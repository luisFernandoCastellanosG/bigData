{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Taller básico sobre WebScraping y MONGOdb\n",
        "----\n",
        "\n",
        "\n",
        "1.   Instalar librerías\n",
        "2.   Crear Doom\n",
        "3.   Recorrer los href\n",
        "*  si es otra pagina ingresar y buscar todos los href que lleven a un PDF\n",
        "*  Descargar cada PDF y extraer texto\n",
        "3.   Crear una Bd\n",
        "4.   Crear colecciones   \n",
        "\n"
      ],
      "metadata": {
        "id": "tHApgJCpMeC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.trabajar con google Drive\n"
      ],
      "metadata": {
        "id": "3qyDUUIeh0Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# habilitamos drive de google desde colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro2GWUEaix6_",
        "outputId": "85134115-e6db-4c1c-aa91-2f54c2a177b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Instalar librerías"
      ],
      "metadata": {
        "id": "SYvuSQKtNajN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 lxml"
      ],
      "metadata": {
        "id": "EWKPxjsJNeXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f530b19-232d-436c-bee4-8ccf9bd3b343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Crear doom inicial"
      ],
      "metadata": {
        "id": "ekK6U-wCkBH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.minsalud.gov.co/Normativa/Paginas/normativa.aspx\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "# encuentre el div con class 'containerblanco'\n",
        "container_div = soup.find('div', class_='containerblanco')\n",
        "\n",
        "# encuentre todos los hypervinculos  dentro del div\n",
        "if container_div:\n",
        "    hyperlinks = container_div.find_all('a')\n",
        "    for link in hyperlinks:\n",
        "        print(link.get('href'))\n",
        "else:\n",
        "    print(\"NO pude encontrar el div con la clase 'containerblanco'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuCSoDF3kG_0",
        "outputId": "23b702c5-21e5-46e1-c265-539673e34056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "/Portada/index.html\n",
            "/Normativa/Paginas/decreto-unico-minsalud-780-de-2016.aspx\n",
            "/Normativa/Paginas/actos-administrativos.aspx\n",
            "http://url.minsalud.gov.co/r9qd8\n",
            "/Paginas/Norm_Conceptos.aspx\n",
            "/Normativa/Paginas/Notificaciones-por-aviso.aspx\n",
            "/Ministerio/Institucional/Paginas/asuntos-juridicos.aspx\n",
            "/Paginas/Norm_Boletines.aspx\n",
            "/Institucional/Paginas/asuntos-juridicos.aspx\n",
            "/Normativa/Paginas/Proyectos-de-actos-administrativos.aspx\n",
            "/Paginas/normograma.aspx\n",
            "/Normativa/Paginas/agenda-regulatoria.aspx\n",
            "/Normativa/Paginas/informe-global-de-participacion-ciudadana.aspx\n",
            "/Normativa/Paginas/analisis-de-impacto-normativo.aspx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Crear JSON recorriendo el DOOM y el de los hyperlinks con extensión aspx\n",
        "\n",
        "\n",
        "---\n",
        "Busque dentro de cada DOOM el div con class \"containerblanco\" y extraiga los hipervinculos\n"
      ],
      "metadata": {
        "id": "w9akSmkwPoXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from urllib.parse import urljoin\n",
        "import os\n",
        "\n",
        "def extract_links(url):\n",
        "    \"\"\"extraer link's internos (PDF y ASPX de una URL)\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "        container_div = soup.find('div', class_='containerblanco')\n",
        "        links = []\n",
        "        if container_div:\n",
        "            for link in container_div.find_all('a'):\n",
        "                href = link.get('href')\n",
        "                if href:\n",
        "                    full_url = urljoin(url, href)\n",
        "                    # Check if the link is within the specified domain\n",
        "                    if full_url.startswith(\"https://www.minsalud.gov.co/Normativa/\"):\n",
        "                        if full_url.endswith('.pdf'):\n",
        "                            links.append({'url': full_url, 'type': 'pdf'})\n",
        "                        elif full_url.endswith('.aspx'):\n",
        "                             links.append({'url': full_url, 'type': 'aspx'})\n",
        "        return links\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Load links from the existing JSON file or initialize if not found\n",
        "json_file_path = '/content/drive/MyDrive/UniversidadCentral/Maestría_en_Analítica_de_Datos/Bigdata/Ejercicios_de_clase/DataBase/Web_scraping/links_minSalud.json'\n",
        "if os.path.exists(json_file_path):\n",
        "    try:\n",
        "        with open(json_file_path, 'r') as f:\n",
        "            json_data = json.load(f)\n",
        "        all_links = json_data.get(\"links\", [])\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Warning: {json_file_path} contains invalid JSON. Initializing with empty links.\")\n",
        "        all_links = []\n",
        "else:\n",
        "    print(f\"{json_file_path} not found. Creating a new file.\")\n",
        "    all_links = []\n",
        "\n",
        "# Filter initial ASPX links to visit based on the domain\n",
        "aspx_links_to_visit = [link['url'] for link in all_links if link['type'] == 'aspx' and link['url'].startswith(\"https://www.minsalud.gov.co/Normativa/\")]\n",
        "visited_aspx_links = set()\n",
        "\n",
        "# Add initial URL if the json file was empty\n",
        "if not all_links:\n",
        "  initial_url = \"https://www.minsalud.gov.co/Normativa/Paginas/normativa.aspx\"\n",
        "  all_links = extract_links(initial_url)  #Extraer los link de la pagina inicial\n",
        "  #depurar el listado (solo dejar lo aspx y pdf)\n",
        "  aspx_links_to_visit = [link['url'] for link in all_links if link['type'] == 'aspx' and link['url'].startswith(\"https://www.minsalud.gov.co/Normativa/\")]\n",
        "\n",
        "\n",
        "# limpiar los link's para que solo queden los que estan en el dominio\n",
        "all_links = [link for link in all_links if link['url'].startswith(\"https://www.minsalud.gov.co/Normativa/\")]\n",
        "\n",
        "\n",
        "while aspx_links_to_visit:\n",
        "    current_aspx_url = aspx_links_to_visit.pop(0)\n",
        "    if current_aspx_url not in visited_aspx_links:\n",
        "        visited_aspx_links.add(current_aspx_url)\n",
        "        print(f\"Visiting: {current_aspx_url}\")\n",
        "        new_links = extract_links(current_aspx_url)\n",
        "        for link in new_links:\n",
        "            if link not in all_links:\n",
        "                all_links.append(link)\n",
        "                if link['type'] == 'aspx':\n",
        "                    aspx_links_to_visit.append(link['url'])\n",
        "\n",
        "# Create a JSON object\n",
        "json_output = {\"links\": all_links}\n",
        "\n",
        "# Save the JSON to a file\n",
        "with open(json_file_path, 'w') as f:\n",
        "    json.dump(json_output, f, indent=4)\n",
        "\n",
        "print(\"finalizado extracción de links y PDF\")"
      ],
      "metadata": {
        "id": "86hbY-FKT8kM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc48594c-33ba-4f2e-f7ce-8f4a47eccee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/web_scraping_min_salud.json not found. Creating a new file.\n",
            "Visiting: https://www.minsalud.gov.co/Normativa/Paginas/decreto-unico-minsalud-780-de-2016.aspx\n",
            "Visiting: https://www.minsalud.gov.co/Normativa/Paginas/actos-administrativos.aspx\n",
            "Visiting: https://www.minsalud.gov.co/Normativa/Paginas/Notificaciones-por-aviso.aspx\n",
            "Visiting: https://www.minsalud.gov.co/Normativa/Paginas/Proyectos-de-actos-administrativos.aspx\n",
            "Visiting: https://www.minsalud.gov.co/Normativa/Paginas/agenda-regulatoria.aspx\n",
            "Visiting: https://www.minsalud.gov.co/Normativa/Paginas/informe-global-de-participacion-ciudadana.aspx\n",
            "Visiting: https://www.minsalud.gov.co/Normativa/Paginas/analisis-de-impacto-normativo.aspx\n",
            "Visiting: https://www.minsalud.gov.co/Normativa/Paginas/normativa.aspx\n",
            "Filtered JSON file 'extracted_links_filtered.json' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.recorrer el JSON y descargar todos los PDF"
      ],
      "metadata": {
        "id": "mRR5Db9UdIV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Define the directory to save PDFs\n",
        "pdf_dir = \"/content/drive/MyDrive/UniversidadCentral/Maestría_en_Analítica_de_Datos/Bigdata/Ejercicios_de_clase/DataBase/Web_scraping/minSalud_pdfs\"\n",
        "os.makedirs(pdf_dir, exist_ok=True)\n",
        "\n",
        "# Load the JSON file\n",
        "json_file_path = '/content/drive/MyDrive/UniversidadCentral/Maestría_en_Analítica_de_Datos/Bigdata/Ejercicios_de_clase/DataBase/Web_scraping/links_minSalud.json'\n",
        "try:\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        json_data = json.load(f)\n",
        "    all_links = json_data.get(\"links\", [])\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {json_file_path} not found. Please run the previous step to create it.\")\n",
        "    all_links = []\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Error: {json_file_path} contains invalid JSON.\")\n",
        "    all_links = []\n",
        "\n",
        "\n",
        "# Download PDF files\n",
        "for link in all_links:\n",
        "    if link['type'] == 'pdf':\n",
        "        pdf_url = link['url']\n",
        "        try:\n",
        "            response = requests.get(pdf_url, stream=True)\n",
        "            response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "            # Extract filename from the URL\n",
        "            filename = os.path.join(pdf_dir, pdf_url.split('/')[-1])\n",
        "\n",
        "            with open(filename, 'wb') as pdf_file:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    pdf_file.write(chunk)\n",
        "            print(f\"Downloaded: {filename}\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading {pdf_url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred while processing {pdf_url}: {e}\")\n",
        "\n",
        "print(\"PDF download process completed.\")"
      ],
      "metadata": {
        "id": "Ug8y3ODSdVZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Extraer texto de los PDF's y generar un JSON por cada PDF"
      ],
      "metadata": {
        "id": "n97Xzrr7o7C1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Instalar librerías para hacer OCR sobre archivos pdf para extraer texto en español"
      ],
      "metadata": {
        "id": "vWYXtNXcpPoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando Tesseract OCR y el paquete de ESPAÑOL\n",
        "!apt-get update\n",
        "!apt-get install tesseract-ocr libtesseract-dev tesseract-ocr-spa\n",
        "\n",
        "# Instalar pytesseract and Pillow\n",
        "!pip install pytesseract Pillow\n",
        "\n",
        "!pip install matplotlib-venn\n",
        "\n",
        "!pip install pdfminer.six pdf2image"
      ],
      "metadata": {
        "id": "61Qo5Rw5o_vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Extraer texto de cada PDF descargado y generar un JSON"
      ],
      "metadata": {
        "id": "HLCrTF5irab1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "# Import necessary components from pdfminer.six\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from io import StringIO\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "avuyb2jBwzzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file, trying standard extraction first, then OCR.\n",
        "    Returns the extracted text, the extraction type ('normal', 'OCR', or 'failed'), and a status (True for success, False for failure).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try standard text extraction using pdfminer.six\n",
        "        rsrcmgr = PDFResourceManager()\n",
        "        retstr = StringIO()\n",
        "        laparams = LAParams()\n",
        "        device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
        "        fp = open(pdf_path, 'rb')\n",
        "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "        for page in PDFPage.get_pages(fp, caching=True, check_extractable=True):\n",
        "            interpreter.process_page(page)\n",
        "        text = retstr.getvalue()\n",
        "        fp.close()\n",
        "        device.close()\n",
        "        retstr.close()\n",
        "\n",
        "        if text.strip():  # Check if standard extraction yielded any text\n",
        "            return text, 'normal', True\n",
        "    except Exception as e:\n",
        "        print(f\"Standard text extraction failed for {pdf_path}: {e}\")\n",
        "\n",
        "    # If standard extraction failed or yielded no text, try OCR\n",
        "    try:\n",
        "        # Convert PDF pages to images for OCR\n",
        "        from pdf2image import convert_from_path\n",
        "        images = convert_from_path(pdf_path)\n",
        "        ocr_text = \"\"\n",
        "        for i, image in enumerate(images):\n",
        "            ocr_text += pytesseract.image_to_string(image, lang='spa')\n",
        "        if ocr_text.strip():\n",
        "             return ocr_text, 'OCR', True\n",
        "        else:\n",
        "             print(f\"OCR de texto no fue posible {pdf_path}\")\n",
        "             return \"\", 'OCR', False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"OCR text extraction failed for {pdf_path}: {e}\")\n",
        "        return \"\", 'failed', False # Indicate that both methods failed\n",
        "\n",
        "\n",
        "# Define input and output directories\n",
        "pdf_input_dir = '/content/drive/MyDrive/UniversidadCentral/Maestría_en_Analítica_de_Datos/Bigdata/Ejercicios_de_clase/DataBase/Web_scraping/minSalud_pdfs'\n",
        "json_output_dir = '/content/drive/MyDrive/UniversidadCentral/Maestría_en_Analítica_de_Datos/Bigdata/Ejercicios_de_clase/DataBase/Web_scraping/minSalud_json'\n",
        "error_log_path = '/content/drive/MyDrive/UniversidadCentral/Maestría_en_Analítica_de_Datos/Bigdata/Ejercicios_de_clase/DataBase/Web_scraping/error_extract_texto.json'\n",
        "\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(json_output_dir, exist_ok=True)\n",
        "\n",
        "# Get list of PDF files\n",
        "pdf_files = [f for f in os.listdir(pdf_input_dir) if f.endswith('.pdf')]\n",
        "\n",
        "# List to store names of files with extraction errors\n",
        "error_files = []\n",
        "\n",
        "# Process each PDF file\n",
        "for i, pdf_file in enumerate(pdf_files):\n",
        "    pdf_path = os.path.join(pdf_input_dir, pdf_file)\n",
        "    extracted_text, extraction_type, success = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    if success:\n",
        "        # Create JSON data\n",
        "        json_data = {\n",
        "            \"archivo\": pdf_file,\n",
        "            \"fecha\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "            \"tipo_extracion\": extraction_type,\n",
        "            \"texto\": extracted_text\n",
        "        }\n",
        "\n",
        "        # Save JSON to file\n",
        "        json_filename = f\"minsalud{i+1}.json\"\n",
        "        json_path = os.path.join(json_output_dir, json_filename)\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(json_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Processed {pdf_file} -> Saved to {json_filename} (Extraction Type: {extraction_type})\")\n",
        "    else:\n",
        "        error_files.append(pdf_file)\n",
        "        print(f\"Failed to extract text from {pdf_file} (Extraction Type: {extraction_type})\")\n",
        "\n",
        "\n",
        "# Save the list of error files to a JSON file\n",
        "with open(error_log_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump({\"failed_files\": error_files}, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"PDF text extraction and JSON creation completed.\")\n",
        "print(f\"Error log saved to {error_log_path}\")"
      ],
      "metadata": {
        "id": "1ex7oLwqr3Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. cargar json a mongoAtlas"
      ],
      "metadata": {
        "id": "e5R09Ua41oGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 librerias"
      ],
      "metadata": {
        "id": "R5SUyMTI1uIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo\n",
        "!pip install py2neo"
      ],
      "metadata": {
        "id": "7ITEpeEG1w4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Establecer la conexión a mongoAtlas"
      ],
      "metadata": {
        "id": "0rsGnm0H1y5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "#reemplazar el <db_password>\n",
        "uri = \"mongodb+srv://DbCentral:DbCentral2025@cluster0.vhltza7.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "client = MongoClient(uri)\n",
        "client.stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_pyqdiK12VF",
        "outputId": "ca8ae887-cfa0-4fd6-a4bb-7acb5bce0e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Database(MongoClient(host=['ac-m2bjmyo-shard-00-01.vhltza7.mongodb.net:27017', 'ac-m2bjmyo-shard-00-02.vhltza7.mongodb.net:27017', 'ac-m2bjmyo-shard-00-00.vhltza7.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='Cluster0', authsource='admin', replicaset='atlas-q336d9-shard-0', tls=True), 'stats')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Crear una base de datos (minSalud)"
      ],
      "metadata": {
        "id": "6xn79ZmK3vBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_name='minsalud'\n",
        "db = client[db_name]  #crear una base de datos\n",
        "\n",
        "#crear una coleccion\n",
        "collection_name='normatividad'\n",
        "collection = db[collection_name]\n",
        "\n",
        "print(f\" base de datps {db_name}, coleccion {collection_name} creadas exitosamente\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yS6CDRl31ok",
        "outputId": "6eaf8dca-6a6e-4ef7-c709-dbc710e06049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " base de datps minsalud, coleccion normatividad creadas exitosamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4 cargar archivos json a coleccion"
      ],
      "metadata": {
        "id": "aJpzgy8w3-Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pymongo.errors import PyMongoError\n",
        "\n",
        "json_input_dir = '/content/drive/MyDrive/UniversidadCentral/Maestría_en_Analítica_de_Datos/Bigdata/Ejercicios_de_clase/DataBase/Web_scraping/minSalud_json'\n",
        "\n",
        "# Get list of JSON files\n",
        "json_files = [f for f in os.listdir(json_input_dir) if f.endswith('.json')]\n",
        "\n",
        "# Counter for successfully loaded files\n",
        "loaded_count = 0\n",
        "failed_files = []\n",
        "\n",
        "print(f\"Starting to load {len(json_files)} JSON files into MongoDB...\")\n",
        "\n",
        "for json_file in json_files:\n",
        "    json_path = os.path.join(json_input_dir, json_file)\n",
        "    try:\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Insert the data into the collection\n",
        "        insert_result = collection.insert_one(data)\n",
        "\n",
        "        if insert_result.inserted_id:\n",
        "            print(f\"Successfully loaded {json_file}. Inserted ID: {insert_result.inserted_id}\")\n",
        "            loaded_count += 1\n",
        "        else:\n",
        "            print(f\"Failed to insert data from {json_file}. No inserted_id returned.\")\n",
        "            failed_files.append(json_file)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {json_path}\")\n",
        "        failed_files.append(json_file)\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Invalid JSON in file {json_file}\")\n",
        "        failed_files.append(json_file)\n",
        "    except PyMongoError as e:\n",
        "        print(f\"MongoDB error while loading {json_file}: {e}\")\n",
        "        failed_files.append(json_file)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing {json_file}: {e}\")\n",
        "        failed_files.append(json_file)\n",
        "\n",
        "print(f\"\\nFinished loading JSON files.\")\n",
        "print(f\"Successfully loaded {loaded_count} files.\")\n",
        "if failed_files:\n",
        "    print(f\"Failed to load {len(failed_files)} files: {failed_files}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt9ekiaR4Ba0",
        "outputId": "1ccb1070-2867-4908-a0f8-af669d63d182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to load 54 JSON files into MongoDB...\n",
            "Successfully loaded minsalud1.json. Inserted ID: 68deb0989dfe00d7f8068e2a\n",
            "Successfully loaded minsalud2.json. Inserted ID: 68deb0999dfe00d7f8068e2b\n",
            "Successfully loaded minsalud3.json. Inserted ID: 68deb0999dfe00d7f8068e2c\n",
            "Successfully loaded minsalud4.json. Inserted ID: 68deb0999dfe00d7f8068e2d\n",
            "Successfully loaded minsalud5.json. Inserted ID: 68deb0999dfe00d7f8068e2e\n",
            "Successfully loaded minsalud6.json. Inserted ID: 68deb0999dfe00d7f8068e2f\n",
            "Successfully loaded minsalud7.json. Inserted ID: 68deb0999dfe00d7f8068e30\n",
            "Successfully loaded minsalud8.json. Inserted ID: 68deb0999dfe00d7f8068e31\n",
            "Successfully loaded minsalud9.json. Inserted ID: 68deb0999dfe00d7f8068e32\n",
            "Successfully loaded minsalud10.json. Inserted ID: 68deb0999dfe00d7f8068e33\n",
            "Successfully loaded minsalud11.json. Inserted ID: 68deb0999dfe00d7f8068e34\n",
            "Successfully loaded minsalud12.json. Inserted ID: 68deb0999dfe00d7f8068e35\n",
            "Successfully loaded minsalud13.json. Inserted ID: 68deb0999dfe00d7f8068e36\n",
            "Successfully loaded minsalud14.json. Inserted ID: 68deb0999dfe00d7f8068e37\n",
            "Successfully loaded minsalud15.json. Inserted ID: 68deb0999dfe00d7f8068e38\n",
            "Successfully loaded minsalud16.json. Inserted ID: 68deb0999dfe00d7f8068e39\n",
            "Successfully loaded minsalud17.json. Inserted ID: 68deb0999dfe00d7f8068e3a\n",
            "Successfully loaded minsalud18.json. Inserted ID: 68deb0999dfe00d7f8068e3b\n",
            "Successfully loaded minsalud19.json. Inserted ID: 68deb0999dfe00d7f8068e3c\n",
            "Successfully loaded minsalud20.json. Inserted ID: 68deb0999dfe00d7f8068e3d\n",
            "Successfully loaded minsalud21.json. Inserted ID: 68deb0999dfe00d7f8068e3e\n",
            "Successfully loaded minsalud22.json. Inserted ID: 68deb0999dfe00d7f8068e3f\n",
            "Successfully loaded minsalud23.json. Inserted ID: 68deb0999dfe00d7f8068e40\n",
            "Successfully loaded minsalud24.json. Inserted ID: 68deb0999dfe00d7f8068e41\n",
            "Successfully loaded minsalud25.json. Inserted ID: 68deb0999dfe00d7f8068e42\n",
            "Successfully loaded minsalud26.json. Inserted ID: 68deb0999dfe00d7f8068e43\n",
            "Successfully loaded minsalud27.json. Inserted ID: 68deb0999dfe00d7f8068e44\n",
            "Successfully loaded minsalud28.json. Inserted ID: 68deb0999dfe00d7f8068e45\n",
            "Successfully loaded minsalud29.json. Inserted ID: 68deb0999dfe00d7f8068e46\n",
            "Successfully loaded minsalud30.json. Inserted ID: 68deb0999dfe00d7f8068e47\n",
            "Successfully loaded minsalud31.json. Inserted ID: 68deb0999dfe00d7f8068e48\n",
            "Successfully loaded minsalud32.json. Inserted ID: 68deb0999dfe00d7f8068e49\n",
            "Successfully loaded minsalud33.json. Inserted ID: 68deb0999dfe00d7f8068e4a\n",
            "Successfully loaded minsalud34.json. Inserted ID: 68deb0999dfe00d7f8068e4b\n",
            "Successfully loaded minsalud35.json. Inserted ID: 68deb0999dfe00d7f8068e4c\n",
            "Successfully loaded minsalud36.json. Inserted ID: 68deb0999dfe00d7f8068e4d\n",
            "Successfully loaded minsalud37.json. Inserted ID: 68deb0999dfe00d7f8068e4e\n",
            "Successfully loaded minsalud38.json. Inserted ID: 68deb0999dfe00d7f8068e4f\n",
            "Successfully loaded minsalud39.json. Inserted ID: 68deb0999dfe00d7f8068e50\n",
            "Successfully loaded minsalud40.json. Inserted ID: 68deb0999dfe00d7f8068e51\n",
            "Successfully loaded minsalud41.json. Inserted ID: 68deb0999dfe00d7f8068e52\n",
            "Successfully loaded minsalud42.json. Inserted ID: 68deb0999dfe00d7f8068e53\n",
            "Successfully loaded minsalud43.json. Inserted ID: 68deb09a9dfe00d7f8068e54\n",
            "Successfully loaded minsalud44.json. Inserted ID: 68deb09a9dfe00d7f8068e55\n",
            "Successfully loaded minsalud45.json. Inserted ID: 68deb09a9dfe00d7f8068e56\n",
            "Successfully loaded minsalud46.json. Inserted ID: 68deb09a9dfe00d7f8068e57\n",
            "Successfully loaded minsalud47.json. Inserted ID: 68deb09a9dfe00d7f8068e58\n",
            "Successfully loaded minsalud48.json. Inserted ID: 68deb09a9dfe00d7f8068e59\n",
            "Successfully loaded minsalud49.json. Inserted ID: 68deb09a9dfe00d7f8068e5a\n",
            "Successfully loaded minsalud50.json. Inserted ID: 68deb09a9dfe00d7f8068e5b\n",
            "Successfully loaded minsalud51.json. Inserted ID: 68deb09a9dfe00d7f8068e5c\n",
            "Successfully loaded minsalud52.json. Inserted ID: 68deb09a9dfe00d7f8068e5d\n",
            "Successfully loaded minsalud53.json. Inserted ID: 68deb09a9dfe00d7f8068e5e\n",
            "Successfully loaded minsalud54.json. Inserted ID: 68deb09a9dfe00d7f8068e5f\n",
            "\n",
            "Finished loading JSON files.\n",
            "Successfully loaded 54 files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "ujruLR_6olFc"
      }
    }
  ]
}